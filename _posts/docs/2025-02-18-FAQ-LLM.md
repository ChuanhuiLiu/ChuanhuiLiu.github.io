---
title: "FAQs of Large Language Models"
last_modified_at: 2025-02-23
header:
  image: /assets/images/LLMtrees.png
  teaser: "/assets/images/illustration/QA.png"
  caption: "Credit: [**Yang et. al. 2023**](https://arxiv.org/abs/2304.13712)"
  excerpt: "The core concepts of LLM in a Q&A format."
permalink: /:categories/:title/
breadcrumbs: true
categories:
  - Notes
tags: 
  - LLM
toc: true
---


The notes aims to provide a *systematic* and *concise* introduction in *Q&A form* to the fundamentals of Large Language Models, serving as a knowledge base for beginners or interviewees. It is motivated by the fact that existing general ML FAQs repos([1](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/tree/main), [2](https://github.com/andrewekhalel/MLQuestions), [3](https://github.com/khangich/machine-learning-interview)) are not up-to-date in Natural Language Processing tasks, due to fast advancements in **LLMs**. 

After reading this blog, you should be able to understand the following diagrams.

| ![pic]({{ site.url }}{{ site.baseurl }}/assets/images/LLMarch.png) | 
|:--:| 
| *Applications of LLM as a service. Credit: [**Nicole Choi**](https://github.blog/ai-and-ml/llms/the-architecture-of-todays-llm-applications/)* |

| ![pic]({{ site.url }}{{ site.baseurl }}/assets/images/LLMtraining.png) | 
|:--:| 
| *Diagram of training Large Language model. Credit: [**Chip Huyen**](https://huyenchip.com/2023/05/02/rlhf.html)* |

| ![pic]({{ site.url }}{{ site.baseurl }}/assets/images/LLMopt.png) | 
|:--:| 
| *Improving LLM Performance . Credit: [**Miguel Carreira Neves**](https://www.tensorops.ai/post/prompt-eng-vs-rag-vs-fine-tuning-what-do-you-need)* |

| ![pic]({{ site.url }}{{ site.baseurl }}/assets/images/LLMflow.png) | 
|:--:| 
| *Diagram of knowledge distillation. Credit: [**Xu et. al 2024**](https://arxiv.org/pdf/2402.13116)* |

### 1. what is word embedding? 

"embedding" originates from mathematics, referring to how one instance of a structure/space/set is contained within another instance.
As a method of natural language processing, word embedding maps the space of natural language into *a normed vector space*, such as Euclidean space.
It is a numerical representation of words and phrases that aims to capture not only *the meaning of words and phrases*, 
but also *reflect the semantic relationships by their norm-induced distance*. 

Such requirement of word embedding, referred to as linguistic regularities ([Mikolov et al, 2013](https://aclanthology.org/N13-1090.pdf))
depends on a [manifold hypothesis](https://ieeexplore.ieee.org/abstract/document/6789755) on the space of natural language, 
assuming that high dimensional numerical representations of words or phrases tend to lie near a low dimensional manifold.
  
More read: [1](https://www.cs.princeton.edu/courses/archive/spring20/cos598C/lectures/lec2-word-embeddings.pdf) [2](https://medium.com/@harsh.vardhan7695/a-comprehensive-guide-to-word-embeddings-in-nlp-ee3f9e4663ed)